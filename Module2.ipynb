{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "csvSchema = StructType([\n",
    "  StructField(\"timestamp\", StringType(), False),\n",
    "  StructField(\"site\", StringType(), False),\n",
    "  StructField(\"requests\", IntegerType(), False)\n",
    "])\n",
    "\n",
    "csvFile = \"/mnt/training/wikipedia/pageviews/pageviews_by_second.tsv\"\n",
    "\n",
    "csvDF = (spark.read\n",
    "  .option('header', 'true')\n",
    "  .option('sep', \"\\t\")\n",
    "  .schema(csvSchema)\n",
    "  .csv(csvFile)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = userhome + \"/pageviews_by_second.parquet\"\n",
    "print(\"Output location: \" + fileName)\n",
    "\n",
    "(csvDF.write                       # Our DataFrameWriter\n",
    "  .option(\"compression\", \"snappy\") # One of none, snappy, gzip, and lzo\n",
    "  .mode(\"overwrite\")               # Replace existing files\n",
    "  .parquet(fileName)               # Write DataFrame to Parquet files\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "\n",
    "# The students will actually need to do this in two steps.\n",
    "fileName = \"dbfs:/mnt/training/wikipedia/clickstream/2015_02_clickstream.tsv\"\n",
    "\n",
    "# The first step will be to use inferSchema = true \n",
    "# It's the only way to figure out what the column and data types are\n",
    "(spark.read\n",
    "  .option(\"sep\", \"\\t\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .csv(fileName)\n",
    "  .printSchema()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# The second step is to create the schema\n",
    "schema = StructType([\n",
    "    StructField(\"prev_id\", IntegerType(), False),\n",
    "    StructField(\"curr_id\", IntegerType(), False),\n",
    "    StructField(\"n\", IntegerType(), False),\n",
    "    StructField(\"prev_title\", StringType(), False),\n",
    "    StructField(\"curr_title\", StringType(), False),\n",
    "    StructField(\"type\", StringType(), False)\n",
    "])\n",
    "\n",
    "fileName = \"dbfs:/mnt/training/wikipedia/clickstream/2015_02_clickstream.tsv\"\n",
    "\n",
    "#The third step is to read the data in with the user-defined schema\n",
    "testDF = (spark.read\n",
    "  .option(\"sep\", \"\\t\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .schema(schema)\n",
    "  .csv(fileName)\n",
    ")\n",
    "\n",
    "testDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%python\n",
    "# ****************************************************************************\n",
    "# Utility method to count & print the number of records in each partition.\n",
    "# ****************************************************************************\n",
    "\n",
    "def printRecordsPerPartition(df):\n",
    "  def countInPartition(iterator): yield __builtin__.sum(1 for _ in iterator)\n",
    "  results = (df.rdd                   # Convert to an RDD\n",
    "    .mapPartitions(countInPartition)  # For each partition, count\n",
    "    .collect()                        # Return the counts to the driver\n",
    "  )\n",
    "  \n",
    "  print(\"Per-Partition Counts\")\n",
    "  i = 0\n",
    "  for result in results: \n",
    "    i = i + 1\n",
    "    print(\"#{}: {:,}\".format(i, result))\n",
    "  \n",
    "# ****************************************************************************\n",
    "# Utility to count the number of files in and size of a directory\n",
    "# ****************************************************************************\n",
    "\n",
    "def computeFileStats(path):\n",
    "  bytes = 0\n",
    "  count = 0\n",
    "\n",
    "  files = dbutils.fs.ls(path)\n",
    "  \n",
    "  while (len(files) > 0):\n",
    "    fileInfo = files.pop(0)\n",
    "    if (fileInfo.isDir() == False):               # isDir() is a method on the fileInfo object\n",
    "      count += 1\n",
    "      bytes += fileInfo.size                      # size is a parameter on the fileInfo object\n",
    "    else:\n",
    "      files.extend(dbutils.fs.ls(fileInfo.path))  # append multiple object to files\n",
    "      \n",
    "  return (count, bytes)\n",
    "\n",
    "# ****************************************************************************\n",
    "# Utility method to cache a table with a specific name\n",
    "# ****************************************************************************\n",
    "\n",
    "def cacheAs(df, name, level):\n",
    "  from pyspark.sql.utils import AnalysisException\n",
    "  print(\"WARNING: The PySpark API currently does not allow specification of the storage level - using MEMORY-ONLY\")\n",
    "  \n",
    "  try: spark.catalog.uncacheTable(name)\n",
    "  except AnalysisException: None\n",
    "  \n",
    "  df.createOrReplaceTempView(name)\n",
    "  spark.catalog.cacheTable(name)\n",
    "  #spark.catalog.cacheTable(name, level)\n",
    "  return df\n",
    "\n",
    "\n",
    "# ****************************************************************************\n",
    "# Simplified benchmark of count()\n",
    "# ****************************************************************************\n",
    "\n",
    "def benchmarkCount(func):\n",
    "  import time\n",
    "  start = float(time.time() * 1000)                    # Start the clock\n",
    "  df = func()\n",
    "  total = df.count()                                   # Count the records\n",
    "  duration = float(time.time() * 1000) - start         # Stop the clock\n",
    "  return (df, total, duration)\n",
    "\n",
    "\n",
    "# ****************************************************************************\n",
    "# Utility method to wait until the stream is read\n",
    "# ****************************************************************************\n",
    "\n",
    "def untilStreamIsReady(name):\n",
    "  queries = list(filter(lambda query: query.name == name, spark.streams.active))\n",
    "\n",
    "  if len(queries) == 0:\n",
    "    print(\"The stream is not active.\")\n",
    "\n",
    "  else:\n",
    "    while (queries[0].isActive and len(queries[0].recentProgress) == 0):\n",
    "      pass # wait until there is any type of progress\n",
    "\n",
    "    if queries[0].isActive:\n",
    "      print(\"The stream is active and ready.\")\n",
    "    else:\n",
    "      print(\"The stream is not active.\")\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%python\n",
    "testResults = dict()\n",
    "\n",
    "def toHash(value):\n",
    "  from pyspark.sql.functions import hash\n",
    "  from pyspark.sql.functions import abs\n",
    "  values = [(value,)]\n",
    "  return spark.createDataFrame(values, [\"value\"]).select(abs(hash(\"value\")).cast(\"int\")).first()[0]\n",
    "\n",
    "def clearYourResults(passedOnly = True):\n",
    "  whats = list(testResults.keys())\n",
    "  for what in whats:\n",
    "    passed = testResults[what][0]\n",
    "    if passed or passedOnly == False : del testResults[what]\n",
    "\n",
    "def validateYourSchema(what, df, expColumnName, expColumnType = None):\n",
    "  label = \"{}:{}\".format(expColumnName, expColumnType)\n",
    "  key = \"{} contains {}\".format(what, label)\n",
    "\n",
    "  try:\n",
    "    actualType = df.schema[expColumnName].dataType.typeName()\n",
    "    \n",
    "    if expColumnType == None: \n",
    "      testResults[key] = (True, \"validated\")\n",
    "      print(\"\"\"{}: validated\"\"\".format(key))\n",
    "    elif actualType == expColumnType:\n",
    "      testResults[key] = (True, \"validated\")\n",
    "      print(\"\"\"{}: validated\"\"\".format(key))\n",
    "    else:\n",
    "      answerStr = \"{}:{}\".format(expColumnName, actualType)\n",
    "      testResults[key] = (False, answerStr)\n",
    "      print(\"\"\"{}: NOT matching ({})\"\"\".format(key, answerStr))\n",
    "  except:\n",
    "      testResults[what] = (False, \"-not found-\")\n",
    "      print(\"{}: NOT found\".format(key))\n",
    "      \n",
    "def validateYourAnswer(what, expectedHash, answer):\n",
    "  # Convert the value to string, remove new lines and carriage returns and then escape quotes\n",
    "  if (answer == None): answerStr = \"null\"\n",
    "  elif (answer is True): answerStr = \"true\"\n",
    "  elif (answer is False): answerStr = \"false\"\n",
    "  else: answerStr = str(answer)\n",
    "\n",
    "  hashValue = toHash(answerStr)\n",
    "  \n",
    "  if (hashValue == expectedHash):\n",
    "    testResults[what] = (True, answerStr)\n",
    "    print(\"\"\"{} was correct, your answer: {}\"\"\".format(what, answerStr))\n",
    "  else:\n",
    "    testResults[what] = (False, answerStr)\n",
    "    print(\"\"\"{} was NOT correct, your answer: {}\"\"\".format(what, answerStr))\n",
    "\n",
    "def summarizeYourResults():\n",
    "  html = \"\"\"<html><body><div style=\"font-weight:bold; font-size:larger; border-bottom: 1px solid #f0f0f0\">Your Answers</div><table style='margin:0'>\"\"\"\n",
    "\n",
    "  whats = list(testResults.keys())\n",
    "  whats.sort()\n",
    "  for what in whats:\n",
    "    passed = testResults[what][0]\n",
    "    answer = testResults[what][1]\n",
    "    color = \"green\" if (passed) else \"red\" \n",
    "    passFail = \"passed\" if (passed) else \"FAILED\" \n",
    "    html += \"\"\"<tr style='font-size:larger; white-space:pre'>\n",
    "                  <td>{}:&nbsp;&nbsp;</td>\n",
    "                  <td style=\"color:{}; text-align:center; font-weight:bold\">{}</td>\n",
    "                  <td style=\"white-space:pre; font-family: monospace\">&nbsp;&nbsp;{}</td>\n",
    "                </tr>\"\"\".format(what, color, passFail, answer)\n",
    "  html += \"</table></body></html>\"\n",
    "  displayHTML(html)\n",
    "\n",
    "def logYourTest(path, name, value):\n",
    "  value = float(value)\n",
    "  if \"\\\"\" in path: raise ValueError(\"The name cannot contain quotes.\")\n",
    "  \n",
    "  dbutils.fs.mkdirs(path)\n",
    "\n",
    "  csv = \"\"\" \"{}\",\"{}\" \"\"\".format(name, value).strip()\n",
    "  file = \"{}/{}.csv\".format(path, name).replace(\" \", \"-\").lower()\n",
    "  dbutils.fs.put(file, csv, True)\n",
    "\n",
    "def loadYourTestResults(path):\n",
    "  from pyspark.sql.functions import col\n",
    "  return spark.read.schema(\"name string, value double\").csv(path)\n",
    "\n",
    "def loadYourTestMap(path):\n",
    "  rows = loadYourTestResults(path).collect()\n",
    "  \n",
    "  map = dict()\n",
    "  for row in rows:\n",
    "    map[row[\"name\"]] = row[\"value\"]\n",
    "  \n",
    "  return map\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%python\n",
    "from pyspark.sql.functions import expr, col, from_unixtime, to_date\n",
    "\n",
    "dbutils.fs.rm(userhome + \"/delta/iot-events/\", True)\n",
    "\n",
    "streamingEventPath = \"/mnt/training/structured-streaming/events/\"\n",
    "\n",
    "(spark\n",
    "  .read\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .json(streamingEventPath)\n",
    "  .withColumn(\"date\", to_date(from_unixtime(col(\"time\").cast(\"Long\"),\"yyyy-MM-dd\")))\n",
    "  .withColumn(\"deviceId\", expr(\"cast(rand(5) * 100 as int)\"))\n",
    "  .repartition(200)\n",
    "  .write\n",
    "  .mode(\"overwrite\")\n",
    "  .format(\"delta\")\n",
    "  .partitionBy(\"date\")\n",
    "  .save(userhome + \"/delta/iot-events/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "#**********************************\n",
    "# VARIOUS UTILITY FUNCTIONS\n",
    "#**********************************\n",
    "\n",
    "def assertSparkVersion(expMajor, expMinor):\n",
    "  major = spark.conf.get(\"com.databricks.training.spark.major-version\")\n",
    "  minor = spark.conf.get(\"com.databricks.training.spark.minor-version\")\n",
    "\n",
    "  if (int(major) < expMajor) or (int(major) == expMajor and int(minor) < expMinor):\n",
    "    msg = \"This notebook must run on Spark version {}.{} or better, found.\".format(expMajor, expMinor, major, minor)\n",
    "    raise Exception(msg)\n",
    "    \n",
    "  return major+\".\"+minor\n",
    "\n",
    "def assertDbrVersion(expMajor, expMinor):\n",
    "  major = spark.conf.get(\"com.databricks.training.dbr.major-version\")\n",
    "  minor = spark.conf.get(\"com.databricks.training.dbr.minor-version\")\n",
    "\n",
    "  if (int(major) < expMajor) or (int(major) == expMajor and int(minor) < expMinor):\n",
    "    msg = \"This notebook must run on Databricks Runtime (DBR) version {}.{} or better, found.\".format(expMajor, expMinor, major, minor)\n",
    "    raise Exception(msg)\n",
    "    \n",
    "  return major+\".\"+minor\n",
    "\n",
    "def assertIsMlRuntime():\n",
    "  version = spark.conf.get(\"com.databricks.training.dbr.version\")\n",
    "  if \"-ml-\" not in version:\n",
    "    raise Exception(\"This notebook must be ran on a Databricks ML Runtime, found {}.\".format(version))\n",
    "\n",
    "    \n",
    "#**********************************\n",
    "# GET AZURE DATASOURCE\n",
    "#**********************************\n",
    "\n",
    "\n",
    "def getAzureDataSource(): \n",
    "  datasource = spark.conf.get(\"com.databricks.training.azure.datasource\").split(\"\\t\")\n",
    "  source = datasource[0]\n",
    "  sasEntity = datasource[1]\n",
    "  sasToken = datasource[2]\n",
    "  return (source, sasEntity, sasToken)\n",
    "\n",
    "    \n",
    "#**********************************\n",
    "# GET EXPERIMENT ID\n",
    "#**********************************\n",
    "\n",
    "def getExperimentId():\n",
    "  return spark.conf.get(\"com.databricks.training.experimentId\")\n",
    "\n",
    "#**********************************\n",
    "# INIT VARIOUS VARIABLES\n",
    "#**********************************\n",
    "\n",
    "username = spark.conf.get(\"com.databricks.training.username\", \"unknown-username\")\n",
    "userhome = spark.conf.get(\"com.databricks.training.userhome\", \"unknown-userhome\")\n",
    "\n",
    "import sys\n",
    "pythonVersion = spark.conf.set(\"com.databricks.training.python-version\", sys.version[0:sys.version.index(\" \")])\n",
    "\n",
    "None # suppress output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "csvSchema = StructType([\n",
    "  StructField(\"timestamp\", StringType(), False),\n",
    "  StructField(\"site\", StringType(), False),\n",
    "  StructField(\"requests\", IntegerType(), False)\n",
    "])\n",
    "\n",
    "csvFile = \"/mnt/training/wikipedia/pageviews/pageviews_by_second.tsv\"\n",
    "\n",
    "csvDF = (spark.read\n",
    "  .option('header', 'true')\n",
    "  .option('sep', \"\\t\")\n",
    "  .schema(csvSchema)\n",
    "  .csv(csvFile)\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
